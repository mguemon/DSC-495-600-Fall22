{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Topic Modelling and Word Embeddings: How to Create a Topic Model and visualize data](Topic-Modelling-and-Word-Embeddings:-How-to-Create-a-Topic-Model-and-visualize-data)\n",
    "\n",
    "\n",
    "1. [Import Data](#Import-Data)  \n",
    "2. [Preprocessing: Cleaning Data](#Preprocessing:-Cleaning-Data)     \n",
    "      a. [Getting Word Counts](#Getting-Word-Counts)    \n",
    "      b. [Exploration of the data set using Ngrams](#Exploration-of-the-data-set-using-Ngrams)      \n",
    "3. [Word Embeddings](#Word-Embeddings)    \n",
    "      a.[Word2Vec](#Word2Vec)   \n",
    "      b.[t-SNE](#t-SNE)\n",
    "4.[Topic Modeling](#Topic-Modeling)              \n",
    "5.[Latent Dirichlet Allocation (LDA)](#Latent-Dirichlet-Allocation-(LDA))  \n",
    "6.[Non-negative Matrix Factorization (NMF)](#Non-negative-Matrix-Factorization-(NMF))\n",
    "5. [Vizualisation using Wordcloud](#Vizualisation-using-Wordcloud)    \n",
    "      a. [For LDA Topics](#For-LDA-Topics)   \n",
    "      b. [For NMF Topics](#For-NMF-Topics)   \n",
    "6. [Challenge](#Challenge)         \n",
    "\n",
    "# Topic Modelling and Word Embeddings: How to Create a Topic Model and visualize data  \n",
    "\n",
    "There are numerous steps that can be taken to help put all text on equal footing, many of which involve the comparatively simple ideas of substitution or removal. They are, however, no less important to the overall process. \n",
    "\n",
    "Let's go through the different steps we have to take when working on our corpus and we are hoping to understand it. \n",
    "\n",
    "\n",
    "## Import Data\n",
    "\n",
    "I've included an excerpt from [News Category Dataset](https://www.kaggle.com/datasets/rmisra/news-category-dataset/versions/2/data) in the Data Folder as well! This file is called `News_Category_Dataset_v2.json`.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_sm\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize;\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist \n",
    "stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "\n",
    "#suppress all warnings with this\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import Counter\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy for preprocessing\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_csv('merged_songs.csv')\n",
    "# import data\n",
    "df = pd.read_json('News_Category_Dataset_v2.json', lines=True)\n",
    "df.head(3)\n",
    "print(\"the shape of the dataframe is \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[jupyter and pandas display](http://songhuiming.github.io/pages/2017/04/02/jupyter-and-pandas-display/) is a good resource to help use jupyters display with pandas to the fullest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Cleaning Data\n",
    "\n",
    "There are numerous osteps that can be taken to help put all text on equal footing, many of which involve the comparatively simple ideas of substitution or removal. They are, however, no less important to the overall process. These include:   \n",
    "\n",
    "* set all characters to lowercase\n",
    "* remove punctuation (generally part of tokenization, but still worth keeping in mind at this stage, even as confirmation)\n",
    "* remove numbers (or convert numbers to textual representations)\n",
    "* strip white space (also generally part of tokenization)\n",
    "* remove default stop words (general English stop words)\n",
    "\n",
    "But sometimes, it also involves understanding the data and figuring out what we can keep and what we can get rid of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['short_description'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for automatic linebreaks and multi-line cells.\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppress all warnings with this\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['short_description'].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset as you have news headline based on section types such as crime news headline or politics. The next will identify how many news headlines found for each section. Below I will show the formula used to count the news headline based on the section. The values of news headlines vary to the section. \n",
    "\n",
    "The next step will identify the highest three sections in terms of the amount news headline contain. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'category':'Section'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting how many headline each section have \n",
    "df_section_type = df.groupby('Section').count()['headline'].reset_index()\n",
    "df_section_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section distribution\n",
    "ax = df.groupby(\"Section\").count()[\"headline\"].plot(kind=\"bar\", \n",
    "                                                 figsize=(8, 5),\n",
    "                                                 title=\"Headline based on each section\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __highest three sections__:  \n",
    "* politics, \n",
    "* entertainment\n",
    "* Wellness\n",
    "\n",
    "But Because of their size I will choose some of the lowest sections to continue in class, but do so again with the Highest Three Sections. \n",
    "* education\n",
    "* culture & arts\n",
    "* arts\n",
    "* food & drink \n",
    "* black voices \n",
    "* style \n",
    "* home & living \n",
    "* healthy linving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the chart above will subset the dataset to keep news headlines of the highest three category news headlines, which in our case of the highest are Politics, entertainment, and Wellness. Suggesting the data and leaving three categories is getting the highest news headline to get a better result for the model and remove more noises.\n",
    "\n",
    "For your challenge, you will be using thesse Categories\n",
    "\n",
    "But as I mentioned above, so we can accomplish most of what we need in class, I will be choosing some other categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The highest three sections\n",
    "#df = df[df[\"Section\"].isin(['ENTERTAINMENT','POLITICS','WELLNESS'])]\n",
    "#The lowest and diversesections\n",
    "df = df[df[\"Section\"].isin(['EDUCATION','CULTURE & ARTS','ARTS','FOOD & DRINK',\n",
    "                          'BLACK VOICES','STYLE','HOME & LIVING','HEALTHY LIVING' ]) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Nbr_words_headline'] = df['headline'].apply(lambda x:len(str(x).split()))\n",
    "df['Nbr_words_description'] = df['short_description'].apply(lambda x:len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(df['Nbr_words_description'],kde = False,color=\"red\", bins = 100)\n",
    "plt.title(\"Frequency distribution of number of words for each text extracted\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "df['Nbr_words_description'].plot(kind=\"box\")\n",
    "plt.title(\"Word frequency distribution using Box plot\", size = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we clean the headlines and short description by removing all the  punctuation, \n",
    "#removing all that is unnecessary.\n",
    "def cleaned_text(text):\n",
    "    clean = re.sub(\"\\n\",\" \",text)\n",
    "    clean=clean.lower()\n",
    "    clean=re.sub(r\"[~.,%/:;?_&+*=!-]\",\" \",clean)\n",
    "    clean=re.sub(\"[^a-z]\",\" \",clean)\n",
    "    clean=clean.lstrip()\n",
    "    clean=re.sub(\"\\s{2,}\",\" \",clean)\n",
    "    return clean\n",
    "\n",
    "df['headline_cleaned'] = df['headline'].apply(cleaned_text)\n",
    "\n",
    "df['short_description_cleaned'] = df['short_description'].apply(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud=WordCloud(colormap='tab20c',width=600,height=400).generate(str(df[\"headline_cleaned\"]))\n",
    "fig=plt.figure(figsize=(10,15))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cloud,interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud=WordCloud(colormap='viridis',width=600,height=400).generate(str(df[\"short_description_cleaned\"]))\n",
    "fig=plt.figure(figsize=(10,15))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cloud,interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point I will be focusing on the News Headlines to do our data Analysis on it.\n",
    "We will be:\n",
    "* removing stopwords\n",
    "* tokenizing \n",
    "* lemmatizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove stopwords\n",
    "stop=stopwords.words('english')\n",
    "df[\"stop_removed_headline\"]=df['headline_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokenized\"]=df[\"stop_removed_headline\"].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_lemmatizer(text):\n",
    "    lem_text = [WordNetLemmatizer().lemmatize(i,pos='v') for i in text]\n",
    "    return lem_text\n",
    "df[\"lemmatized\"]=df[\"tokenized\"].apply(lambda x: word_lemmatizer(x))\n",
    "df[\"lemmatize_joined\"]=df[\"lemmatized\"].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Number_of_words_for_cleaned'] = df['lemmatize_joined'].apply(lambda x:len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(df['Number_of_words_for_cleaned'],kde = False, color= \"navy\", bins = 100)\n",
    "plt.title(\"Frequency distribution of number of words for each text extracted after removing stopwords and lemmatization\", size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "freq=pd.Series(\" \".join(df[\"lemmatize_joined\"]).split()).value_counts()[:30]\n",
    "freq.plot(kind=\"bar\", color = \"orangered\")\n",
    "plt.title(\"30 most frequent words\",size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of the data set using Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams\n",
    "\n",
    "Ngrams are defined as contiguous sequences of n words. If the number of words has the same meaning as New York , it's called Bigram, and if it's more than two words, then it's considered as a trigram. Figure 12 is considered in Ngram as unigrams, which is only one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_words(df['headline'], 20)\n",
    "\n",
    "df1 = pd.DataFrame(common_words, columns = ['headline' , 'count'])\n",
    "\n",
    "fig = go.Figure([go.Bar(x=df1['headline'], y=df1['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 20 words in news_headline before removing stop words\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_bigram(df[\"headline\"], 20)\n",
    "df3 = pd.DataFrame(common_words, columns = ['bigram' , 'count'])\n",
    "\n",
    "fig = go.Figure([go.Bar(x=df3['bigram'], y=df3['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 20 bigrams in the news headline text after removing stop words and lemmatization\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above shows the top 20 bigrams in the news headline, which is the most frequent words after removing stop words and lemmatization. Comparing the 2 figures, we can see the words more meaningful and not repeated twice, such `new york`, mentioned as one word and `recipe day` as one word. Using bigrams shows to improve our data retrieval to be more meaningful, which in later stage helps to improve the quality of our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_trigram(df[\"headline\"], 20)\n",
    "df4 = pd.DataFrame(common_words, columns = ['trigram' , 'count'])\n",
    "\n",
    "fig = go.Figure([go.Bar(x=df4['trigram'], y=df4['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 20 trigrams in the news headline\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the result of pulling the trigrams as top 20 occurring from our news dataset. Some of the text showing in the graph does not make sense to be as three words‚Äîour observation showing that Bigrams could more useful to use for our dataset.\n",
    "\n",
    "In the last stage of our exploration, we used an entity named recognition to understand more about our data. The technique considered to be an information extraction method in which entities that are present in the text are classified into entity types such as place or organization etc. After running this model will understand the type of entities mentioned in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run the below code in class as it makes too much time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_named_entity_barchart(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    def _get_ner(text):\n",
    "        doc=nlp(text)\n",
    "        return [X.label_ for X in doc.ents]\n",
    "    \n",
    "    ent=text.apply(lambda x : _get_ner(x))\n",
    "    ent=[x for sub in ent for x in sub]\n",
    "    counter=Counter(ent)\n",
    "    count=counter.most_common()\n",
    "    \n",
    "    x,y=map(list,zip(*count))\n",
    "    sns.barplot(x=y,y=x)\n",
    "\n",
    "plot_named_entity_barchart(df['headline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows that PERSON entity is the highest which consider most of the names of people mentioned. The second highest in the ORG, which is short of an organization that was mentioned in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the most recurring person in the data set\n",
    "def ner(text,ent=\"PERSON\"):\n",
    "    doc=nlp(text)\n",
    "    return [X.text for X in doc.ents if X.label_ == ent]\n",
    "\n",
    "gpe=df['headline'].apply(lambda x: ner(x))\n",
    "gpe=[i for x in gpe for i in x]\n",
    "counter=Counter(gpe)\n",
    "\n",
    "x,y=map(list,zip(*counter.most_common(10)))\n",
    "sns.barplot(y,x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows that `Kerry Washington`, the most person mentioned in our dataset, with `Colin Kaepernick`, `Donald Trump` and `Hillary Clinton` following behind. \n",
    "However, we do see that our function did find `Weekly Roundup` and `Valentine` as the Top person, that is most likely because of the writing and the positioning of those words in the headlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the most organisation in the data set \n",
    "def ner(text,ent=\"ORG\"):\n",
    "    doc=nlp(text)\n",
    "    return [X.text for X in doc.ents if X.label_ == ent]\n",
    "\n",
    "gpe=df['headline'].apply(lambda x: ner(x))\n",
    "gpe=[i for x in gpe for i in x]\n",
    "counter=Counter(gpe)\n",
    "\n",
    "x,y=map(list,zip(*counter.most_common(10)))\n",
    "sns.barplot(y,x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "## Word2Vec\n",
    "Word2vec is a two-layer neural net that processes text by ‚Äúvectorizing‚Äù words. Its input is a text corpus and its output is a set of vectors: feature vectors that represent words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep neural networks can understand.\n",
    "\n",
    "Word2vec‚Äôs applications extend beyond parsing sentences in the wild. It can be applied just as well to genes, code, likes, playlists, social media graphs and other verbal or symbolic series in which patterns may be discerned.\n",
    "\n",
    "Why? Because words are simply discrete states like the other data mentioned above, and we are simply looking for the transitional probabilities between those states: the likelihood that they will co-occur.\n",
    "\n",
    "The purpose and usefulness of Word2vec is to group the vectors of similar words together in vectorspace. That is, it detects similarities mathematically. Word2vec creates vectors that are distributed numerical representations of word features, features such as the context of individual words. It does so without human intervention.\n",
    "\n",
    "Given enough data, usage and contexts, Word2vec can make highly accurate guesses about a word‚Äôs meaning based on past appearances. Those guesses can be used to establish a word‚Äôs association with other words (e.g. ‚Äúman‚Äù is to ‚Äúboy‚Äù what ‚Äúwoman‚Äù is to ‚Äúgirl‚Äù), or cluster documents and classify them by topic. Those clusters can form the basis of search, sentiment analysis and recommendations in such diverse fields as scientific research, legal discovery, e-commerce and customer relationship management.\n",
    "\n",
    "The output of the Word2vec neural net is a vocabulary in which each item has a vector attached to it, which can be fed into a deep-learning net or simply queried to detect relationships between words.\n",
    "\n",
    "Want to find out more? [Click here:](https://medium.com/@zafaralibagh6/a-simple-word2vec-tutorial-61e64e38a6a1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = df[\"lemmatize_joined\"].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(all_words,\n",
    "                     min_count=600,\n",
    "                     window=10,\n",
    "                     #size=250,\n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007,\n",
    "                     workers = 4,\n",
    "                     seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the vector representation of the word `recipes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = w2v_model.wv['recipes']\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar words using Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words = w2v_model.wv.most_similar('home')\n",
    "print(sim_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words_2 = w2v_model.wv.most_similar('video')\n",
    "print(sim_words_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words_3 = w2v_model.wv.most_similar('food')\n",
    "print(sim_words_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words_4 = w2v_model.wv.most_similar('photos')\n",
    "print(sim_words_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "\n",
    "t-SNE is something called nonlinear dimensionality reduction. What that means is this algorithm allows us to separate data that cannot be separated by any straight line, let me show you an example:\n",
    "\n",
    "\n",
    "What is exactly t-SNE?\n",
    "\n",
    "t-SNE a non-linear dimensionality reduction algorithm finds patterns in the data based on the similarity of data points with features, the similarity of points is calculated as the conditional probability that a point A would choose point B as its neighbour. It then tries to minimize the difference between these conditional probabilities (or similarities) in higher-dimensional and lower-dimensional space for a perfect representation of data points in 2 or 3 dimensional space.\n",
    "\n",
    "The nearest neighbor accuracy gives the probability that a random point has the same species as its closest neighbor. This would be close to 100% if the points were perfectly grouped according to their species. A high nearest neighbor accuracy implies that the data can be cleanly separated into groups.\n",
    "\n",
    "Any parameters?\n",
    "\n",
    "There are many, but here we will be understanding and implementing an arguably important parameter of t-SNE: perplexity\n",
    "\n",
    "Perplexity is perhaps the most important parameter in t-SNE and can reveal different aspects of the data. Considered loosely, it can be thought of as the balance between preserving the global and the local structure of the data. A more direct way to think about perplexity is that it is the continuous analogy to the KNN (K-Nearest Neighbors) for which we will preserve distances.\n",
    "\n",
    "In most implementations, perplexity defaults to 30. This focuses the attention of t-SNE on preserving the distances to its 30 nearest neighbors and puts virtually no weight on preserving distances to the remaining points. For data sets with a small number of points e.g. 100, this will uncover the global structure quite well since each point will preserve distances to a third of the data set.\n",
    "\n",
    "For larger data sets, e.g. 10,000 points, considering 30 nearest neighbors will likely do a poor job of preserving global structure. Using a higher perplexity value e.g. 500, will do a much better job for of uncovering the global structure. For larger data sets still e.g. 500k or 1 million samples, this is typically not enough and can take quite a long time to run. Luckily, various tricks can be used to improve global structure.\n",
    "\n",
    "Want to find out more? [Click here:](https://www.youtube.com/watch?v=NEaUSP4YerM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot():\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    \n",
    "    # Extracting words and their vectors from our trained model \n",
    "    for word in model.wv.index_to_key:\n",
    "        tokens.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    # Train t-SNE \n",
    "    tsne_model = TSNE(perplexity=45, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "        plt.xlabel(\"dimension 1\")\n",
    "        plt.ylabel(\"dimension 2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that occur atleast 50 times\n",
    "model = Word2Vec(all_words, window=20, min_count=50, workers=4)\n",
    "tsne_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that occur atleast 2 times\n",
    "model = Word2Vec(all_words, window=20, min_count=2, workers=4)\n",
    "tsne_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets more and more difficult to read as the size of dataset increases üòµ   \n",
    "\n",
    "Hence, for __semantic retention__ and better inferences, we will plot only the __similar words__ to a chosen key.   \n",
    "\n",
    "Word2Vector provides us with an inbuilt functionality to give the list of top similar words with respect to the cosine-distanceüí´    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('atlanta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['atlanta', 'holiday', 'health', 'introvert']\n",
    "\n",
    "# this array will contain the vectors(dimension 100) and the labels\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "for word in keys:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for similar_word, _ in model.wv.most_similar(word, topn=30):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(model.wv[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "tsne_model_en_2d = TSNE(perplexity=50, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=8)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"dimension 1\")\n",
    "    plt.ylabel(\"dimension 2\")\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "tsne_plot_similar_words('Similar words chosen keys', keys, embeddings_en_2d, word_clusters, 0.7,\n",
    "                        'similar_words.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Topic modeling is a form of text mining, a way of identifying patterns in a corpus. You take your corpus and run it through a tool which groups words across the corpus into ‚Äòtopics‚Äô."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "__Latent Dirichlet Allocation (LDA)__ and LSA are based on the same underlying assumptions: the distributional hypothesis, (i.e. similar topics make use of similar words) and the statistical mixture hypothesis (i.e. documents talk about several topics) for which a statistical distribution can be determined. The purpose of LDA is mapping each document in our corpus to a set of topics which covers a good deal of the words in the document.\n",
    "\n",
    "![alt text](Schematic-of-LDA-algorithm.png \"Schematic of LDA algorithm (D. Buena√±o-Fernandez et al.  2020)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']): \n",
    "    output = []        \n",
    "    for sent in texts:              \n",
    "        doc = nlp(\" \".join(sent))                             \n",
    "        output.append([token.lemma_ for token in doc if \n",
    "        token.pos_ in allowed_postags])        \n",
    "    return output\n",
    "# function to remove stopwords \n",
    "def remove_stopwords(rev):     \n",
    "    rev_new = \" \".join([i for i in rev if i not in stop_words])      \n",
    "    return rev_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove short words (length < 3) \n",
    "df['headline_cleaned'] = df['headline_cleaned'].apply(lambda x: ' '.join([w for \n",
    "                   w in x.split() if len(w)>2])) \n",
    "# remove stopwords from the text \n",
    "headline = [remove_stopwords(r.split()) for r in df['headline_cleaned']] \n",
    "# make entire text lowercase \n",
    "headline = [r.lower() for r in headline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_headline = pd.Series(headline).apply(lambda x: x.split())\n",
    "print(tokenized_headline[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_2 = lemmatization(tokenized_headline)\n",
    "print(headline_2[1]) # print lemmatized headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics = 5\n",
    "dictionary = corpora.Dictionary(headline_2)\n",
    "doc_term_matrix = [dictionary.doc2bow(headline) for lyric in headline_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "# Creating the object for LDA model using gensim library \n",
    "LDA = gensim.models.ldamodel.LdaModel \n",
    "# Build LDA model \n",
    "lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary,                                     \n",
    "                num_topics=ntopics, random_state=100, chunksize=1000,                                     \n",
    "                passes=50)\n",
    "print('\\nThe LDA_MODEL dataset done in '+'%s seconds'% (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word lists\n",
    "for i in range(0,ntopics):\n",
    "    temp = lda_model.show_topic(i, 10)\n",
    "    terms = []\n",
    "    for term in temp:\n",
    "        terms.append(term)\n",
    "    print(\"\\nTop 10 terms for topic #\" + str(i) + \": \"+ \", \".join([i[0] for i in terms]))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_topics(model, num_topics):\n",
    "    word_dict = {};\n",
    "    for i in range(ntopics):\n",
    "        words = model.show_topic(i, topn = 20);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lda_topics(lda_model, ntopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each document to most prevalent topic\n",
    "lda_topic_assignment = [max(p,key=lambda item: item[1]) for p in lda_model[doc_term_matrix]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Topic_list_cln =[p[0] for p in lda_topic_assignment ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Topic_LDA\"] = Topic_list_cln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative Matrix Factorization (NMF)\n",
    "__Non-Negative Matrix Factorization__ is a statistical method that helps us to reduce the dimension of the input corpora or corpora. Internally, it uses the factor analysis method to give comparatively less weightage to the words that are having less coherence. The way it works is that, NMF decomposes (or factorizes) high-dimensional vectors into a lower-dimensional representation. These lower-dimensional vectors are non-negative which also means their coefficients are non-negative.\n",
    "\n",
    "![alt text](5-Figure1-1.png \"Illustration of NMF model for topic modeling. (Kelsey MacMillan, James D. Wilson. 2017)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', max_features=5000);\n",
    "cln_counts = vectorizer.fit_transform(df['headline_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_cln = TfidfTransformer(smooth_idf=False);\n",
    "x_tfidf_cln = transformer_cln.fit_transform(cln_counts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtfidf_norm_cln = normalize(x_tfidf_cln, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain a NMF model.\n",
    "model_cln = NMF(n_components=ntopics, random_state = 50,init='nndsvd');\n",
    "#fit the model\n",
    "W_mat_cln = model_cln.fit_transform(xtfidf_norm_cln)\n",
    "H_mat_cln = model_cln.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Topic_NMF\"] = np.argmax(W_mat_cln, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(ntopics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nmf_topics(model_cln, ntopics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualisation using Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "stopwords = set(STOPWORDS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For LDA Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Topic_LDA'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5,2, figsize=(21,20))\n",
    "\n",
    "for item  in enumerate(list(df['Topic_LDA'].unique())):\n",
    "    wc = WordCloud(background_color=\"White\",stopwords = stopwords,\n",
    "               max_words=1000, max_font_size= 200,  width=1600, height=800,min_font_size = 10)\n",
    "    wc.generate(\" \".join(df[df['Topic_LDA']== item[1]]['headline']))\n",
    "    \n",
    "    axs[item[1]//2, item[1]%2].set_title(\"Topic %d\" % item[1])\n",
    "    axs[item[1]//2, item[1]%2].imshow(wc, aspect='auto', interpolation='bilinear')\n",
    "    axs[item[1]//2, item[1]%2].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For NMF Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Topic_NMF'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5,2, figsize=(21,20))\n",
    "\n",
    "for item  in enumerate(list(df['Topic_NMF'].unique())):\n",
    "    wc = WordCloud(background_color=\"White\",stopwords = stopwords,\n",
    "               max_words=1000, max_font_size= 200,  width=1600, height=800,min_font_size = 10)\n",
    "    wc.generate(\" \".join(df[df['Topic_NMF']== item[1]]['headline']))\n",
    "    \n",
    "    axs[item[1]//2, item[1]%2].set_title(\"Topic %d\" % item[1])\n",
    "    axs[item[1]//2, item[1]%2].imshow(wc, aspect='auto', interpolation='bilinear')\n",
    "    axs[item[1]//2, item[1]%2].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have done most of this work using the news headlines as they are shorter for in class study.  \n",
    "Using the Top 3 Sections we have found earlier.   \n",
    "Follow the same steps we have here using the Short Description columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
